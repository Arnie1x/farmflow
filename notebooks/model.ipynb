{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "Mj2QSfY9tjLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "hgwCposhtrEt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ehGfHu5thk1",
        "outputId": "4eb4dee7-c31a-4359-e7f8-99b257619265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 nltk pandas\n",
        "\n",
        "import PyPDF2\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Preprocessor"
      ],
      "metadata": {
        "id": "6Dh8ubrcts9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFPreprocessor:\n",
        "    def __init__(self):\n",
        "        # Download required NLTK data\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('punkt_tab')\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Extract raw text from PDF file.\"\"\"\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                # Create PDF reader object\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "                # Extract text from each page\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and normalize the extracted text.\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove page numbers\n",
        "        text = re.sub(r'\\b\\d+\\b(?=\\s*$)', '', text)\n",
        "\n",
        "        # Remove headers and footers (customize patterns based on your PDFs)\n",
        "        text = re.sub(r'^\\s*page\\s+\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove special characters but keep periods for sentence splitting\n",
        "        text = re.sub(r'[^a-z0-9\\s\\.]', '', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def split_into_chunks(self, text, max_tokens=512):\n",
        "        \"\"\"Split text into chunks suitable for training.\"\"\"\n",
        "        # First split into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        current_token_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Rough estimation of tokens (words + punctuation)\n",
        "            sentence_tokens = len(sentence.split())\n",
        "\n",
        "            if current_token_count + sentence_tokens > max_tokens:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence\n",
        "                current_token_count = sentence_tokens\n",
        "            else:\n",
        "                current_chunk += \" \" + sentence\n",
        "                current_token_count += sentence_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def create_training_data(self, input_dir, output_file):\n",
        "        \"\"\"Process all PDFs in a directory and create training data.\"\"\"\n",
        "        pdf_files = list(Path(input_dir).glob('*.pdf'))\n",
        "        all_chunks = []\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            print(f\"Processing {pdf_file}\")\n",
        "\n",
        "            # Extract and clean text\n",
        "            raw_text = self.extract_text_from_pdf(pdf_file)\n",
        "            cleaned_text = self.clean_text(raw_text)\n",
        "\n",
        "            # Split into chunks\n",
        "            chunks = self.split_into_chunks(cleaned_text)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        # Create training examples\n",
        "        training_data = []\n",
        "        for chunk in all_chunks:\n",
        "            # Create example with context\n",
        "            example = {\n",
        "                \"text\": chunk,\n",
        "                \"metadata\": {\n",
        "                    \"source\": \"rice_farming_manual\",\n",
        "                    \"tokens\": len(chunk.split())\n",
        "                }\n",
        "            }\n",
        "            training_data.append(example)\n",
        "\n",
        "        # Save to JSON file\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(training_data, f, indent=2)\n",
        "\n",
        "        print(f\"Created {len(training_data)} training examples\")\n",
        "        return training_data\n",
        "\n",
        "    def analyze_dataset(self, training_data):\n",
        "        \"\"\"Analyze the created dataset.\"\"\"\n",
        "        df = pd.DataFrame([{\n",
        "            'text_length': len(example['text']),\n",
        "            'token_count': example['metadata']['tokens']\n",
        "        } for example in training_data])\n",
        "\n",
        "        stats = {\n",
        "            'total_examples': len(training_data),\n",
        "            'avg_text_length': df['text_length'].mean(),\n",
        "            'avg_tokens': df['token_count'].mean(),\n",
        "            'min_tokens': df['token_count'].min(),\n",
        "            'max_tokens': df['token_count'].max()\n",
        "        }\n",
        "\n",
        "        return stats"
      ],
      "metadata": {
        "id": "gIbuaZrJtwg6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = PDFPreprocessor()\n",
        "\n",
        "# Set your input and output paths\n",
        "input_directory = \"pdfs\"\n",
        "output_file = \"rice_farming_training_data.json\"\n",
        "\n",
        "# Process PDFs and create training data\n",
        "training_data = preprocessor.create_training_data(input_directory, output_file)\n",
        "\n",
        "# Analyze the dataset\n",
        "stats = preprocessor.analyze_dataset(training_data)\n",
        "print(\"\\nDataset Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_OFuQGTt2hR",
        "outputId": "d937c487-4251-4871-f3d9-a79e49198b4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing pdfs/Kenya Rice-Cultivation-Manual.pdf\n",
            "Created 31 training examples\n",
            "\n",
            "Dataset Statistics:\n",
            "total_examples: 31.00\n",
            "avg_text_length: 3125.06\n",
            "avg_tokens: 473.45\n",
            "min_tokens: 162.00\n",
            "max_tokens: 512.00\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}