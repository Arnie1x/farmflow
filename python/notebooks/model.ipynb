{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ehGfHu5thk1",
    "outputId": "4eb4dee7-c31a-4359-e7f8-99b257619265",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIbuaZrJtwg6"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, \\\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## GPT-2 Small ('gpt2'): 124 million parameters.\n",
    "## GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n",
    "## GPT-2 Large ('gpt2-large'): 774 million parameters.\n",
    "## GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n",
    "\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Your custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.text = f.read().splitlines()\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            self.text[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\")\n",
    "        tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "# Load data\n",
    "data = CustomDataset(tokenizer, \"data/Kenya-Rice-Cultivation-Manual.txt\", 128)\n",
    "\n",
    "# Create a data collator that will dynamically pad the sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5, # Increse for more training from the fine-tuning data\n",
    "    learning_rate=1e-4,  # Decrease the learning rate for smaller fine-tuning data\n",
    "    output_dir='./results',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    evaluation_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    "    eval_dataset=None,  # You can specify an evaluation dataset here\n",
    "    data_collator=data_collator,  # Add the data collator here\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_OFuQGTt2hR",
    "outputId": "d937c487-4251-4871-f3d9-a79e49198b4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pdfs/Kenya Rice-Cultivation-Manual.pdf\n",
      "Created 31 training examples\n",
      "\n",
      "Dataset Statistics:\n",
      "total_examples: 31.00\n",
      "avg_text_length: 3125.06\n",
      "avg_tokens: 473.45\n",
      "min_tokens: 162.00\n",
      "max_tokens: 512.00\n"
     ]
    }
   ],
   "source": [
    "# Ensure your model is in evaluation mode\n",
    "# to disable dropout layers\n",
    "model.eval()\n",
    "\n",
    "# Create a prompt text for the model to complete\n",
    "prompt_text = \"Plants require at least 16 elements for normal growth and for completion of their life cycle.\"\n",
    "\n",
    "# Tokenize the prompt text and convert to tensor\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "attention_mask = tokenizer(\n",
    "    prompt_text, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "# Move input_ids and attention_mask tensor to GPU\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Generate text from the model\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_length=1000,\n",
    "    num_beams=5,\n",
    "    temperature=1.5,\n",
    "    top_k=50,\n",
    "    do_sample=True  # Enable sampling to consider temperature setting\n",
    ")\n",
    "\n",
    "# Decode the generated text back to string\n",
    "generated_text = tokenizer.decode(output[0],\n",
    "                                  skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retireve embeddings\n",
    "input_text= \"Cuidad Acuna, Mexico\"\n",
    "input_tokens = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Ensure tokens are on the same device as the model\n",
    "input_tokens = {k: v.to(device) for k, v in input_tokens.items()}\n",
    "\n",
    "# Forward pass, get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_tokens, output_hidden_states=True)\n",
    "\n",
    "# Only take the hidden states (ignore other outputs)\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "## If you want the embeddings from the last layer of the model:\n",
    "last_layer_embeddings = hidden_states[-1]\n",
    "\n",
    "## the last_layer_embeddings tensor obtained from the\n",
    "# GPT-2 model's forward method is 3D\n",
    "\n",
    "# Mean pool the last_layer_embeddings (across the sequence length dimension)\n",
    "mean_pooled = last_layer_embeddings.mean(dim=1)\n",
    "\n",
    "mean_pooled_embedding =  mean_pooled.squeeze(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_pooled_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mean_pooled_embedding))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
